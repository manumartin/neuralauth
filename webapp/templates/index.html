{% extends "base_generic.html" %}

{% block content %}

<h1>User identity recognition with Deep Neural Networks</h1>

<h3>Introduction</h3>

<p>This site is built as a demo application that showcases how <b>Deep Neural Networks</b> can be used to detect
fraudulent or illegal users by just analyzing their mouse input patterns.</p>

<p>Here I give a little introduction to explain some aspects of how neural networks work and how they can be applied to this problem in particular. In the sections <a href="/deep/">C&amp;C</a> and <a href="/chess/">Chess</a> above, you can interact with the network by playing two different games to generate mouse input. You can name different usage sessions and then see if the network is able to tell them apart. For example two different persons can generate input and the network should be able to distinguish them. Keep in mind this is a work in progress. Sometimes it is apparent that the network is detecting different patterns and why but other times the results are a little more confusing.</p>

<h3>Neural networks</h3>

<p>Neural networks are one of the many types of algorithms in the world of <b>Machine Learning</b>. They allow us to automatize intuitive tasks that traditionally only expert humans were capable of doing. Sometimes the accuracy of these networks is better than that of many expert humans. In addition to their power, Neural Networks are special in the sense that as a solution they don't need to be completely engineered by hand like other complex algorithms of the past. Instead they are generated directly from data.</p>

<p>A Neural Network is a collection of artificial "neurons" connected to each other. These artificial neurons are very simplified mathematical models based on the real biological ones. They are just little <b>pattern detectors</b>. By connecting many neurons together in a big network we can detect complex patterns in data. When a full network is built we can use it to understand previously unseen data and make extrapolations and predictions. We can even use them to generate new data similar to the one we used to train the network.</p>

<p>In other programming paradigms we explicitly state how our algorithm works with a lot of detail. With machine learning or neural networks it doesn't work like that. In this paradigm the algorithm is mostly generated by the data. You just specify some general architecture like, for example, a concrete type of neural network of a given size and then the training data refines the network until its performance is good enough to consider the problem solved.</p>

<h3>A concrete example: image classification</h3>

<p>In the problem of image classification, a big network is trained on many images and when the process ends it is able to classify previously unseen images. It has been discovered that state of the art artificial neural networks work in some ways like the human visual system does and although they are different, they are doing some fundamentally similar processing on the data. Its a little like comparing how birds and airplanes fly, they both do it differently but in the end they both have wings that push against the air.</p>

<p>When an image is fed to an image recognition network, the first layers of the network detect simple patterns, like edges, horizontal or vertical lines, colors etc. The output of these first neurons is then fed to a higher layer of neurons which in turn detects more complex patterns like small forms. This process continues until the data reaches the higher layers which detect more abstract stuff. In the case of face recognition some layers detect individual facial features like eyes, lips or noses while the higher layers detect full faces or even <a href="https://www.newscientist.com/article/dn7567-why-your-brain-has-a-jennifer-aniston-cell/">unique faces</a>.</p>

<!--<h3>Finding a suitable network architecture</h3>

<p>In machine learning in general the problem is always about finding a model that explains some data. In the case of image classification we want to find a mapping between images and their correct categories. Mathematically there are an infinite amount of these mappings but in practice only some of them can be found in a realistic amount of time. The algorithm that we use to find one of those functions is called <b>bayesian inference</b> and in spite of the fancy name it is very simple.<p>

<p>Bayesian inference works in a similar way to the process a detective follows to understand a crime scene. When he arrives at the scene he finds various evidences, he then proceeds to elaborate different hypotheses that explain the evidence. To do this he uses his previous prior knowledge about crime scenes and about the world in general. He knows that some explanations are more probable than others, he also knows how probable those explanations are given the available evidence. After deliberating he starts to feel than one of the possible explanations is the most probable. If he finds more evidence he can then change his mind and prefer another explanation or make refinements to the one he was commited to.</p>

<p>In the case of image classification the first step could be to select neural networks as a starting hypothesis. We are guessing that a neural network not only can explain the data but that we can find a concrete network to do it in a practical amount of time. There are an infinite amount of neural networks though, we need to refine our hypothesis and select one specific kind of neural network that is more suited to images. That specific kind exists and is called a <b>Deep Convolutional Network</b>. Again, there are an infinite number of DCN's, we have to specify the number of layers that we want, the depth and the width of the network, how many neurons it will have etc. Even when we have taken all of those decisions we still have to initialize
each individual neuron. One more time there are an infinite amount of ways of initializing them since their parameters or weights are floating point values. There are initializations that are known to work better than others though, we just have to select one of them.

<p>After all this work our neural network still won't do anything. If we feed an image to it in its current state it will output random useless stuff. We need to find some way to tune each individual weight to improve our hypothesis. This is when the training phase starts.</p>

<h3>Training the neural network</h3>

<p>How do we know how to adjust so many different neurons? An algorithm called <b>Stochastic Gradient Descent</b> can help us do this. The algorithm involves first calculating the output that the untrained network produces for a given example of data. Once we have this output we can calculate how far it is from being correct. This measure of how far the network is from being correct is called the error or loss. When we have the error we can begin tunning the parameters starting in the last layer. Since we know how changing the input given to those neurons affects the output of the network we know in which direction we have to modify the parameters such that our error decreases. Once we have tuned the latest layer we do the same with lower layers in a recursive way. This process of updating the weights or parameters starting on the higher layers is called <b>backpropagation</b></p>

<p>In our case we are going to work with a dataset composed of mouse input events and our objective is to detect if the activity of the user is considered normal or anomalous.</p>
!-->
<h3>User recognition network</h3>

<p>To train a network for user recoginition we first need a suitable amount of data. In the case of this demo I used the <a href="https://github.com/balabit/Mouse-Dynamics-Challenge">Balabit Mouse Dynamics Challenge dataset</a>. This dataset is composed of a lot of data recorded from 10 different users. Training with it, my best model achieved an accuracy of 92%. That means that it would (on average) correctly guess which user has produced a given input sequence 92 out of 100 times. Here you can see an example of the data. Each input event is labeled with a user_id that identifies the user that generated it:</p>

<img class="img-responsive center-block" src="/static/home/example-dataset.png" alt="Dataset used in the demo"/>

<p>Below we can see the actual training process of the user classification network. The Y axis represents the accuracy of the network while the X axis is the number of training iterations, how many times we use the whole dataset to train the network. The blue line is the accuracy on the training data while the green line is the accuracy on data not seen previously by the network:</p>.

<img class="img-responsive center-block" src="/static/home/training.png" alt="Accuracy of a neural network while training"/>

<h3>Hyperparameter tuning</h3>

<p> There are many decisions that we can take about the shape of our network before starting to train. These decisions involve adjusting parameters that are called <b>hyperparameters</b>. Searching for the correct hyperparameters is kind of a trial and error task. Whenever we want to test how a change in a given hyperparameter will affect the accuracy of a network we are forced to restart the training from scracth. Because of this, hyperparameter searching is computationally hard. For example, while training the network used in this demo I tested 40 different network configurations, each with different hyperparameters. I spent nearly 30 hours in an amazon AWS GPU compute instance to do it. Once trained I could visualize which ones performed better and select one for further training. Here we can see the best performing networks grouped by different criteria:</p>

<img src="/static/home/best_models.png" class="figure-img img-responsive img-fluid center-block" alt="Comparisons of different models by varying criteria.">

<p>After I explored all these possibilities one of the models, the 18th, looked slightly better in all respects to all the other ones. I then trained this model again for a longer time and it eventually reached the 92% accuracy level.</p>

<h3>Visualizing the performance of the network</h3> 

<p>Deep neural networks are commonly critiziced as being too opaque and difficult to understand. However, we have certain ways to generate useful visualizations. One of those ways is using a technique called <b>TSNE</b>. TSNE generates a graph in which we can see the degree to which some categories are mistaken with other categories. For example in the graph below we can see that input sequences corresponding to the user 0 (dark blue) are almost always perfectly identified as belonging to that user. Input for other users is sometimes mistaken, for example the input from the user 8 (orange) is sometimes mistaken as belonging to the user 3 (cyan):</p>

<img class="img-responsive center-block" src="/static/home/tsne.png" alt="TSNE visualization"/>

<p>Another useful visualization is what is called a <b>Confusion Matrix</b>. The Confusion Matrix also shows how good our network is at comparing certain users to every other user.</p>

<img class="img-responsive center-block" src="/static/home/confusion.png" alt="Confusion matrix"/>

<h3> The demo </h3>

<p> The demo is composed of two interactive applications, a popular <a href="deep/">real time strategy game</a> and a <a href="chess/">chess game</a>. In both of them you can record the mouse input and assign a name to each recorded session. While the input of each session is recorded a Confusion Matrix graph will be shown. This graph
will show how the input of a user is related to the input of every other user.

<img class="img-responsive center-block" src="/static/home/interface.png" alt="Demo interface"/>

<p> To use the application we first start by selecting a user name <b>(2)</b>. Then we can press the start recording button <b>(4)</b>. After this we can play the game and the input will be sent to the server and analyzed periodically.
The confusion matrix graph <b>(5)</b> will be constanly updated to reflect how each user differs from every other user. If a value of a given cell is near 1 it means that the pair of users identified by
the row and the column look very similar to the network. If the value is near 0 they look very different.</p>

<p>The reset state button <b>(6)</b> will allow us to start from scratch and erase the information about all the previously analyzed users. The Minimum value textbox <b>(3)</b> is useful to tune the color escale of the graph. Sometimes the network will think all the users are very similar and all the scores will be near 9. This will
make all the colors appear dark blue because the color escale is from 0 to 10. If we set the minimum value to 0.9 we will be able to better see the small diferences between the users.</p>


<h3> Some tests </h3>

<p><b>Circles and horizontal lines</b> Here we have recorded input four different times, making lines and circles with the mouse. We can see that although the network considers all the inputs to be very similar, it still thinks lines and lines2 are more similar between them than to circles. The difference is small though and we have to adjust the color scale to 0.96 to be able to see it.</p>

<img class="img-responsive center-block" src="/static/home/minvalue.png" alt="Demo interface"/>

<h3>How the application is made</h3>

<p>The application frontend is made on <b>Javascript</b>, <b>Jquery</b> and <b>Bootstrap</b>. The backend is made using <b>Python</b>, <b>Django</b> and <b>Django REST Framework</b>. Persistence is done with <b>SQLite</b> and pickle. The neural network is executed with <b>Tensorflow</b> in a separate process. The backend communicates with this process by using <b>Pyro4</b>. The confusion matrix that appears in the web is plotted using <b>matplotlib</b>. The TSNE visualization is made with a <b>scikit-learn</b> model.</p>

<p>The chess game uses <a href="http://chessboardjs.com/">chessboardjs</a> while the c&amp;c one uses an <b>amazing</b> port of c&amp;c made by <a href="https://www.adityaravishankar.com/">Aditya Ravi Shankar</a>. You can find it <a href="https://www.adityaravishankar.com/projects/games/command-and-conquer/">here</a>.</p>

<p>The neural network was trained using <b>Tensorflow</b> and <b>Keras</b> in an Amazon AWS compute instance. During the investigations to train the network I used <b>Jupyter notebooks</b> a lot, as well as <b>Pandas</b>, <b>scikit-learn</b> and <b>numpy</b></p>

<p>The "production" server runs on an Amazon AWS instance using <b>Ubuntu</b> linux, <b>Nginx</b> and <b>gunicorn</b> for WSGI</p>




{% endblock %}
